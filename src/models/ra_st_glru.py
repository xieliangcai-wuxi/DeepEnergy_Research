import torch
import torch.nn as nn
from .glru import GLRU
from .retrieval_attention import RetrievalAttention
from .revin import RevIN

class RA_ST_GLRU(nn.Module):
    def __init__(self, num_nodes, in_features, d_model, layers, out_len, top_k, target_idx, use_retrieval=True, dropout=0.1):
        super().__init__()
        
        self.d_model = d_model
        self.out_len = out_len
        self.target_idx = target_idx
        self.in_features = in_features
        self.use_retrieval = use_retrieval
        
        self.revin = RevIN(in_features)

        self.input_proj_current = nn.Sequential(
            nn.Linear(in_features + 768, d_model),
            nn.LayerNorm(d_model),
            nn.Dropout(dropout)
        )
        
        if self.use_retrieval:
            self.input_proj_sim = nn.Sequential(
                nn.Linear(in_features, d_model),
                nn.LayerNorm(d_model),
                nn.Dropout(dropout)
            )

        self.glru_layers = nn.ModuleList([
            GLRU(d_model, dropout) for _ in range(layers)
        ])
        
        if self.use_retrieval:
            self.retrieval_attn = RetrievalAttention(d_model, top_k, dropout)
            self.fusion_gate = nn.Sequential(
                nn.Linear(d_model * 2, d_model),
                nn.Sigmoid() 
            )

        self.output_head = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 2, out_len)
        )

    def forward(self, x_current, x_sim, debug=False):
        """
        å®Œæ•´çš„å‰å‘ä¼ æ’­é€»è¾‘ (SOTA ä¸¥è°¨ç‰ˆ)
        åŒ…å«ï¼šRevIN -> No Masking -> Neural Net -> Shortcut -> Rigorous Denorm
        """
        (x_num, x_text) = x_current
        
        if debug: print("\nğŸ” [Model Internals] Start Forward Pass...")

        # ==========================================
        # Step 1: RevIN Normalization (å®‰æ£€å…¥å£)
        # ==========================================
        # æ¶ˆé™¤éå¹³ç¨³æ€§ã€‚æ³¨æ„ï¼šè¿™é‡ŒåŒ…å«äº† Affine Transform (ä¹˜ weight åŠ  bias)
        x_num_norm = self.revin(x_num, mode='norm')
        
        if debug:
            print(f"   1. Input Norm | Mean: {x_num_norm.mean():.4f} | Std: {x_num_norm.std():.4f}")

        # ==========================================
        # Step 2: Main Branch (ç¥ç»ç½‘ç»œä¸»è·¯)
        # ==========================================
        # ğŸš¨ å…³é”®ï¼šæ—  Maskingï¼ä¿ç•™å®Œæ•´è§†åŠ›ï¼Œè®©æ¨¡å‹çœ‹åˆ°è¶‹åŠ¿ã€‚
        x_fused = torch.cat([x_num_norm, x_text], dim=-1)
        x_emb = self.input_proj_current(x_fused)
        
        # GLRU æå–æ—¶åºç‰¹å¾
        h_seq = x_emb
        for layer in self.glru_layers:
            h_seq = layer(h_seq)
        context = h_seq[:, -1, :] 
        
        # RAG æ£€ç´¢å¢å¼º
        if self.use_retrieval:
            b, k, l, f = x_sim.shape
            x_sim_flat = x_sim.view(b * k, l, f)
            x_sim_emb = self.input_proj_sim(x_sim_flat)
            x_sim_vec = x_sim_emb.mean(dim=1)
            keys_values = x_sim_vec.view(b, k, self.d_model)
            
            retrieval_out = self.retrieval_attn(context.unsqueeze(1), keys_values).squeeze(1)
            g = self.fusion_gate(torch.cat([context, retrieval_out], dim=-1))
            h_final = context + g * retrieval_out
        else:
            h_final = context

        # MLP Head é¢„æµ‹æ®‹å·® (åœ¨å½’ä¸€åŒ–ç©ºé—´ä¸‹)
        pred_residual_norm = self.output_head(h_final)

        # ==========================================
        # Step 3: Direct Method / Shortcut (ç‰©ç†æ·å¾„)
        # ==========================================
        # è¿™å°±æ˜¯ä½ è¦æ‰¾çš„â€œç›´æ¥æ–¹æ³•â€ï¼
        # é€»è¾‘ï¼šç›´æ¥æˆªå– normalized input çš„æœ€å 24 ä¸ªç‚¹
        # æ„ä¹‰ï¼šå‡è®¾å½’ä¸€åŒ–åçš„æ˜å¤© = å½’ä¸€åŒ–åçš„æ˜¨å¤©
        baseline_norm = x_num_norm[:, -self.out_len:, self.target_idx]
        
        # èåˆï¼šæ·å¾„ + æ®‹å·®
        final_pred_norm = baseline_norm + pred_residual_norm
        
        if debug:
            print(f"   2. Pred(Norm) | Mean: {final_pred_norm.mean():.4f} | Std: {final_pred_norm.std():.4f}")

        # ==========================================
        # Step 4: RevIN Denormalization (ä¸¥è°¨åå½’ä¸€åŒ–)
        # ==========================================
        # å¿…é¡»å…ˆé€†è½¬ Affineï¼Œå†é€†è½¬ Mean/Std
        
        # A. é€†è½¬ä»¿å°„å˜æ¢ (Reverse Affine)
        # å…¬å¼: x = (x - bias) / weight
        if self.revin.affine:
            # å–å‡º Target åˆ—å¯¹åº”çš„æ ‡é‡å‚æ•°
            target_weight = self.revin.affine_weight[self.target_idx]
            target_bias = self.revin.affine_bias[self.target_idx]
            
            # å¹¿æ’­è®¡ç®— (Batch, 24) - Scalar
            final_pred_norm = (final_pred_norm - target_bias) / (target_weight + 1e-10)

        # B. é€†è½¬ç»Ÿè®¡é‡ (Reverse Stats)
        # å…¬å¼: x = x * std + mean
        # å–å‡º Target åˆ—å¯¹åº”çš„ç»Ÿè®¡é‡ [Batch, 1, F] -> [Batch, 1]
        target_mean = self.revin.mean[:, :, self.target_idx]
        target_std = self.revin.stdev[:, :, self.target_idx]
        
        # å¹¿æ’­è®¡ç®— (Batch, 24) * (Batch, 1)
        final_pred = final_pred_norm * target_std + target_mean
        
        if debug:
            print(f"   3. Final Output | Mean: {final_pred.mean():.4f} | Std: {final_pred.std():.4f}")
            print("âœ… [Model Internals] Forward Pass Complete.\n")

        # æ¢å¤å½¢çŠ¶ [Batch, Out_Len, 1]
        return final_pred.unsqueeze(-1)