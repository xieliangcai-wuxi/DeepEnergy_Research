import torch
import torch.nn as nn
from .glru import GLRU
from .retrieval_attention import RetrievalAttention
from .revin import RevIN

class RA_ST_GLRU(nn.Module):
    """
    [Final SOTA Architecture: Zero-Init Residual]
    ç­–ç•¥å˜æ›´ï¼š
    1. ç§»é™¤ Maskingï¼šä¿è¯è®­ç»ƒå’Œæµ‹è¯•æ•°æ®åˆ†å¸ƒä¸€è‡´ã€‚
    2. é›¶åˆå§‹åŒ– (Zero-Init)ï¼šå¼ºåˆ¶æ¨¡å‹ä» Baseline (4.19%) èµ·è·‘ï¼Œåªå­¦ä¹ åå·®ã€‚
    """
    def __init__(self, num_nodes, in_features, d_model, layers, out_len, top_k, target_idx, use_retrieval=True, dropout=0.1):
        super().__init__()
        
        # --- Configs ---
        self.d_model = d_model
        self.out_len = out_len
        self.target_idx = target_idx
        self.in_features = in_features
        self.use_retrieval = use_retrieval
        
        # --- 1. RevIN ---
        self.revin = RevIN(in_features, affine=True)

        # --- 2. Projections ---
        self.input_proj_current = nn.Sequential(
            nn.Linear(in_features + 768, d_model),
            nn.LayerNorm(d_model),
            nn.Dropout(dropout)
        )
        
        if self.use_retrieval:
            self.input_proj_sim = nn.Sequential(
                nn.Linear(in_features, d_model),
                nn.LayerNorm(d_model),
                nn.Dropout(dropout)
            )

        # --- 3. Backbone ---
        self.glru_layers = nn.ModuleList([
            GLRU(d_model, dropout) for _ in range(layers)
        ])
        
        # --- 4. RAG ---
        if self.use_retrieval:
            self.retrieval_attn = RetrievalAttention(d_model, top_k, dropout)
            self.fusion_gate = nn.Sequential(
                nn.Linear(d_model * 2, d_model),
                nn.Sigmoid() 
            )

        # --- 5. Output Heads (å…³é”®ä¿®æ”¹) ---
        
        # Head A: Residual Content
        # æœ€åä¸€å±‚ Linear åˆå§‹åŒ–ä¸º 0ï¼Œç¡®ä¿åˆå§‹è¾“å‡ºä¸º 0
        self.output_head = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model * 2, out_len)
        )
        
        # Head B: Confidence Gate
        # åˆå§‹åŒ–ä¸ºè®© Gate æ¥è¿‘ 0 (å®Œå…¨ä¿¡ä»» Shortcut)
        self.confidence_gate = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.Tanh(),
            nn.Linear(d_model // 2, out_len), 
            nn.Sigmoid() 
        )
        
        # ğŸš¨ã€å¿…æ€æŠ€ã€‘é›¶åˆå§‹åŒ– (Zero Initialization)
        # å¼ºè¿«æ¨¡å‹ä¸€å¼€å§‹"é—­å˜´"ï¼Œå®Œå…¨ç­‰åŒäº Shortcut
        self._zero_init_head()

    def _zero_init_head(self):
        """
        [ä¿®æ­£ç‰ˆ] é›¶åˆå§‹åŒ–ç­–ç•¥ v2
        ç›®æ ‡ï¼šä¿æŒåˆå§‹ Loss ä½ (4.19%)ï¼ŒåŒæ—¶ä¿è¯æ¢¯åº¦ç•…é€šã€‚
        """
        print("âš¡ [Init] Applying Zero-Initialization (Gradient-Friendly Version)...")
        
        # 1. æ®‹å·®å†…å®¹å±‚ï¼šå¿…é¡»å…¨ 0
        # è¿™æ · Neural Output = 0
        nn.init.zeros_(self.output_head[-1].weight)
        nn.init.zeros_(self.output_head[-1].bias)
        
        # 2. Gate å±‚ï¼šBias è®¾ä¸º 0 (å…³é”®ä¿®æ”¹ï¼)
        # ä¹‹å‰æ˜¯ -5.0 (å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±)
        # ç°åœ¨æ˜¯ 0.0 -> Sigmoid(0) = 0.5 -> æ¢¯åº¦æœ€å¤§ï¼
        # åˆå§‹çŠ¶æ€: Final = Shortcut + 0.5 * 0 = Shortcut (ä¾ç„¶ç¨³ï¼)
        nn.init.xavier_uniform_(self.confidence_gate[-2].weight) # æƒé‡ä¿æŒéšæœºï¼Œæ‰“ç ´å¯¹ç§°æ€§
        nn.init.zeros_(self.confidence_gate[-2].bias) # Bias è®¾ä¸º 0

    def forward(self, x_current, x_sim, debug=False):
        (x_num, x_text) = x_current
        
        if debug: print("\nğŸ” [Model Internals] Start Forward Pass...")

        # 1. RevIN
        x_num_norm = self.revin(x_num, mode='norm')

        # ğŸš¨ã€ä¿®æ”¹ã€‘å½»åº•ç§»é™¤ Masking
        # æ—¢ç„¶æˆ‘ä»¬ç”¨äº† Zero-Initï¼Œå°±ä¸éœ€è¦ Masking æ¥å¼ºè¿«å­¦ä¹ äº†ã€‚
        # è®©æ¨¡å‹çœ‹å®Œæ•´çš„æ•°æ®ï¼Œå»å¯»æ‰¾é‚£å¾®å°çš„è¯¯å·®ã€‚
        x_input_for_net = x_num_norm # .clone() ä¹Ÿä¸éœ€è¦äº†
        
        # 2. Backbone
        x_fused = torch.cat([x_input_for_net, x_text], dim=-1)
        x_emb = self.input_proj_current(x_fused)
        
        h_seq = x_emb
        for layer in self.glru_layers:
            h_seq = layer(h_seq)
        context = h_seq[:, -1, :] 
        
        # 3. RAG
        if self.use_retrieval:
            b, k, l, f = x_sim.shape
            x_sim_flat = x_sim.view(b * k, l, f)
            x_sim_emb = self.input_proj_sim(x_sim_flat)
            x_sim_vec = x_sim_emb.mean(dim=1)
            keys_values = x_sim_vec.view(b, k, self.d_model)
            retrieval_out = self.retrieval_attn(context.unsqueeze(1), keys_values).squeeze(1)
            g = self.fusion_gate(torch.cat([context, retrieval_out], dim=-1))
            h_final = context + g * retrieval_out
        else:
            h_final = context

        # 4. Prediction
        # ç”±äºé›¶åˆå§‹åŒ–ï¼Œä¸€å¼€å§‹è¿™é‡Œè¾“å‡ºå…¨æ˜¯ 0
        pred_residual_content = self.output_head(h_final)
        
        # ç”±äº Bias=-5ï¼Œä¸€å¼€å§‹è¿™é‡Œå…¨æ˜¯ 0.006 (å‡ ä¹ä¸ä¿¡ç¥ç»ç½‘ç»œ)
        gate_score = self.confidence_gate(h_final)
        
        if debug:
            print(f"   2. Neural Raw | Mean: {pred_residual_content.mean():.4f} (Should be ~0)")
            print(f"   âš–ï¸ [Gate Check] Mean Conf: {gate_score.mean():.4f} (Should be ~0)")

        # 5. Fusion
        baseline_norm = x_num_norm[:, -self.out_len:, self.target_idx]
        
        # åˆå§‹çŠ¶æ€ï¼šBaseline + 0 * 0 = Baseline
        final_pred_norm = baseline_norm + (gate_score * pred_residual_content)

        # 6. Denorm
        if self.revin.affine:
            target_weight = self.revin.affine_weight[self.target_idx]
            target_bias = self.revin.affine_bias[self.target_idx]
            final_pred_norm = (final_pred_norm - target_bias) / (target_weight + 1e-10)

        B, L = final_pred_norm.shape
        target_mean = self.revin.mean[:, :, self.target_idx].view(B, 1)
        target_std = self.revin.stdev[:, :, self.target_idx].view(B, 1)
        
        final_pred = final_pred_norm * target_std + target_mean
        
        return final_pred.unsqueeze(-1)